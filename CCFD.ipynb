{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd1ea872",
   "metadata": {},
   "source": [
    "# Credit Card Fraud Detection Project\n",
    "*Author: Rekula Prudhvi Raj*\n",
    "\n",
    "This notebook leverages the [Kaggle Credit Card Fraud Detection dataset](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud?resource=download), which contains **284,807** anonymized transactions with only **492** labeled as fraud (≈0.17%). We’ll walk through:\n",
    "\n",
    "1. **Loading & Exploration** – quick data overview and class distribution  \n",
    "2. **Preprocessing** – scaling, missing-value checks, and feature engineering  \n",
    "3. **Class Imbalance Handling** – using SMOTE to balance the minority class  \n",
    "4. **Model Training** – comparing Logistic Regression, Decision Tree, Random Forest, XGBoost, and HistGradientBoosting  \n",
    "5. **Evaluation** – precision, recall, F1-score, and ROC-AUC metrics with a focus on minimizing false positives  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39e6e509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: install/upgrade xgboost\n",
    "!pip install --upgrade xgboost --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d75407a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports loaded\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "print('Imports loaded')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d598389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (284807, 31)\n",
      "Class\n",
      "0    0.998273\n",
      "1    0.001727\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('creditcard.csv')\n",
    "print('Data shape:', df.shape)\n",
    "print(df['Class'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04fe1fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing done\n"
     ]
    }
   ],
   "source": [
    "# Preprocess: check missing, scale Amount, drop Time\n",
    "assert df.isnull().sum().sum() == 0, 'Missing values!'\n",
    "scaler = StandardScaler()\n",
    "df['Amount'] = scaler.fit_transform(df[['Amount']])\n",
    "df.drop(columns=['Time'], inplace=True)\n",
    "print('Preprocessing done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a04d02e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (227845, 29) Test: (56962, 29)\n"
     ]
    }
   ],
   "source": [
    "X = df.drop(columns=['Class'])\n",
    "y = df['Class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "print('Train:', X_train.shape, 'Test:', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9332dd24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampled train: (454902, 29)\n"
     ]
    }
   ],
   "source": [
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "print('Resampled train:', X_train_res.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab4fa8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(class_weight='balanced', max_iter=500, n_jobs=-1, random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(class_weight='balanced', random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=50, class_weight='balanced', n_jobs=-1, random_state=42),\n",
    "    'XGBoost': XGBClassifier(n_estimators=100, scale_pos_weight=(len(y_train)-sum(y_train))/sum(y_train), n_jobs=-1, use_label_encoder=False, eval_metric='auc', random_state=42),\n",
    "    'HistGradientBoosting': HistGradientBoostingClassifier(max_iter=100, early_stopping=True, random_state=42)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5a0e695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training Logistic Regression ===\n",
      "Logistic Regression ROC AUC: 0.9700\n",
      "\n",
      "=== Training Decision Tree ===\n",
      "Decision Tree ROC AUC: 0.8917\n",
      "\n",
      "=== Training Random Forest ===\n",
      "Random Forest ROC AUC: 0.9753\n",
      "\n",
      "=== Training XGBoost ===\n",
      "XGBoost ROC AUC: 0.9784\n",
      "\n",
      "=== Training HistGradientBoosting ===\n",
      "HistGradientBoosting ROC AUC: 0.9683\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n=== Training {name} ===\")\n",
    "    model.fit(X_train_res, y_train_res)\n",
    "    preds = model.predict(X_test)\n",
    "    proba = model.predict_proba(X_test)[:, 1]\n",
    "    report = classification_report(y_test, preds, output_dict=True)\n",
    "    auc = roc_auc_score(y_test, proba)\n",
    "    results[name] = {\n",
    "        'precision_fraud': report['1']['precision'],\n",
    "        'recall_fraud': report['1']['recall'],\n",
    "        'f1_fraud': report['1']['f1-score'],\n",
    "        'roc_auc': auc\n",
    "    }\n",
    "    print(f\"{name} ROC AUC: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f88dedf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model Comparison ===\n",
      "                      precision_fraud  recall_fraud  f1_fraud   roc_auc\n",
      "XGBoost                      0.480226      0.867347  0.618182  0.978391\n",
      "Random Forest                0.852632      0.826531  0.839378  0.975314\n",
      "Logistic Regression          0.056285      0.918367  0.106070  0.970028\n",
      "HistGradientBoosting         0.470270      0.887755  0.614841  0.968320\n",
      "Decision Tree                0.359813      0.785714  0.493590  0.891653\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "summary_df = pd.DataFrame(results).T\n",
    "print('\\n=== Model Comparison ===')\n",
    "print(summary_df.sort_values('roc_auc', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18d3825-52d8-4ce2-ad12-461337583ab0",
   "metadata": {},
   "source": [
    "## Interpretation of Model Comparison & Recommendations\n",
    "\n",
    "**XGBoost** (AUC: 0.978391, Precision: 0.480226, Recall: 0.867347, F1: 0.618182)  \n",
    "- Highest AUC: best overall discrimination; use when you need top ranking and can accept moderate false positives.\n",
    "\n",
    "**Random Forest** (AUC: 0.975314, Precision: 0.852632, Recall: 0.826531, F1: 0.839378)  \n",
    "- Best balance (highest F1): recommended for all-round performance when you need both few false positives and few missed frauds.\n",
    "\n",
    "**Logistic Regression** (AUC: 0.970028, Precision: 0.056285, Recall: 0.918367, F1: 0.106070)  \n",
    "- Very high recall: use when catching every fraud is critical and you can tolerate many false alarms.\n",
    "\n",
    "**HistGradientBoosting** (AUC: 0.968320, Precision: 0.470270, Recall: 0.887755, F1: 0.614841)  \n",
    "- Fast training with high recall: alternative when you need early stopping and quicker iterations.\n",
    "\n",
    "**Decision Tree** (AUC: 0.891653, Precision: 0.359813, Recall: 0.785714, F1: 0.493590)  \n",
    "- Fastest to train but lowest performance: useful for quick prototyping or when interpretability is most important.\n",
    "\n",
    "**Choosing a model:**  \n",
    "- If you need fewest false alarms (high precision): **Random Forest**.  \n",
    "- If you must catch all fraud (high recall): **Logistic Regression** or **HistGradientBoosting** (then raise threshold to improve precision).  \n",
    "- If you want best overall ranking (AUC): **XGBoost** (consider threshold tuning for precision).  \n",
    "- For rapid prototyping: **Decision Tree**, then upgrade for production.  \n",
    "\n",
    "Consider stacking your top models for even better results!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
